{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "510b7d52",
      "metadata": {
        "id": "510b7d52"
      },
      "source": [
        "# Thermal Neural Networks (Pytorch example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b698f89e",
      "metadata": {
        "id": "b698f89e"
      },
      "source": [
        "This jupyter notebook showcases how to utilize a thermal neural network (TNN) on an exemplary data set.\n",
        "This example is more concise than the TF2 example and standalone (not utilizing the \"tf2utils\" package) but lacks a few convenient mechanics in turn (e.g., validation set, early stopping, plotting helpers etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d500214",
      "metadata": {
        "id": "6d500214"
      },
      "source": [
        "The data set can be downloaded from [Kaggle](https://www.kaggle.com/wkirgsn/electric-motor-temperature).\n",
        "It should be placed in `data/input/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c6d96117",
      "metadata": {
        "id": "c6d96117"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Parameter as TorchParam\n",
        "from torch import Tensor\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ef7289",
      "metadata": {
        "id": "01ef7289"
      },
      "source": [
        "## Data setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c53203e5",
      "metadata": {
        "id": "c53203e5",
        "outputId": "2525e902-68c1-4bf7-9350-b06f2a7b92c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/input/measures_v2.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f8503b1ed1d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"data\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"input\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"measures_v2.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stator_yoke\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stator_tooth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stator_winding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemperature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ambient\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coolant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/input/measures_v2.csv'"
          ]
        }
      ],
      "source": [
        "path_to_csv = Path().cwd() / \"data\" / \"input\" / \"measures_v2.csv\"\n",
        "data = pd.read_csv(path_to_csv)\n",
        "target_cols = [\"pm\", \"stator_yoke\", \"stator_tooth\", \"stator_winding\"]\n",
        "\n",
        "temperature_cols = target_cols + [\"ambient\", \"coolant\"]\n",
        "test_profiles = [60, 62, 74]\n",
        "train_profiles = [p for p in data.profile_id.unique() if p not in test_profiles]\n",
        "profile_sizes = data.groupby(\"profile_id\").agg(\"size\")\n",
        "\n",
        "# normalize\n",
        "non_temperature_cols = [c for c in data if c not in temperature_cols + [\"profile_id\"]]\n",
        "data.loc[:, temperature_cols] /= 200  # deg C\n",
        "data.loc[:, non_temperature_cols] /= data.loc[:, non_temperature_cols].abs().max(axis=0)\n",
        "\n",
        "# extra feats (FE)\n",
        "if {\"i_d\", \"i_q\", \"u_d\", \"u_q\"}.issubset(set(data.columns.tolist())):\n",
        "    extra_feats = {\n",
        "        \"i_s\": lambda x: np.sqrt((x[\"i_d\"] ** 2 + x[\"i_q\"] ** 2)),\n",
        "        \"u_s\": lambda x: np.sqrt((x[\"u_d\"] ** 2 + x[\"u_q\"] ** 2)),\n",
        "    }\n",
        "data = data.assign(**extra_feats)\n",
        "input_cols = [c for c in data.columns if c not in target_cols]\n",
        "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# overwrite. We recommend CPU over GPU here, as that runs faster with pytorch on this data set\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe40138e",
      "metadata": {
        "id": "fe40138e"
      },
      "outputs": [],
      "source": [
        "# Rearrange features\n",
        "input_cols = [c for c in data.columns if c not in target_cols + [\"profile_id\"]]\n",
        "data = data.loc[:, input_cols + [\"profile_id\"] + target_cols]\n",
        "\n",
        "\n",
        "def generate_tensor(profiles_list):\n",
        "    \"\"\"Returns profiles of the data set in a coherent 3D tensor with\n",
        "    time-major shape (T, B, F) where\n",
        "    T : Maximum profile length\n",
        "    B : Batch size = Amount of profiles\n",
        "    F : Amount of input features.\n",
        "\n",
        "    Also returns a likewise-shaped sample_weights tensor, which zeros out post-padded zeros for use\n",
        "    in the cost function (i.e., it acts as masking tensor)\"\"\"\n",
        "    tensor = np.full(\n",
        "        (profile_sizes[profiles_list].max(), len(profiles_list), data.shape[1] - 1),\n",
        "        np.nan,\n",
        "    )\n",
        "    for i, (pid, df) in enumerate(\n",
        "        data.loc[data.profile_id.isin(profiles_list), :].groupby(\"profile_id\")\n",
        "    ):\n",
        "        assert pid in profiles_list, f\"PID is not in {profiles_list}!\"\n",
        "        tensor[: len(df), i, :] = df.drop(columns=\"profile_id\").to_numpy()\n",
        "    sample_weights = 1 - np.isnan(tensor[:, :, 0])\n",
        "    tensor = np.nan_to_num(tensor).astype(np.float32)\n",
        "    tensor = torch.from_numpy(tensor).to(device)\n",
        "    sample_weights = torch.from_numpy(sample_weights).to(device)\n",
        "    return tensor, sample_weights\n",
        "\n",
        "\n",
        "train_tensor, train_sample_weights = generate_tensor(train_profiles)\n",
        "test_tensor, test_sample_weights = generate_tensor(test_profiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db87e5c",
      "metadata": {
        "id": "1db87e5c"
      },
      "source": [
        "## Model declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0e8b21",
      "metadata": {
        "metadata": {},
        "id": "1e0e8b21"
      },
      "outputs": [],
      "source": [
        "class DiffEqLayer(nn.Module):\n",
        "    \"\"\"This class is a container for the computation logic in each step.\n",
        "    This layer could be used for any 'cell', also RNNs, LSTMs or GRUs.\"\"\"\n",
        "\n",
        "    def __init__(self, cell, *cell_args):\n",
        "        super().__init__()\n",
        "        self.cell = cell(*cell_args)\n",
        "\n",
        "    def forward(self, input: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        inputs = input.unbind(0)\n",
        "        outputs = torch.jit.annotate(List[Tensor], [])\n",
        "        for i in range(len(inputs)):\n",
        "            out, state = self.cell(inputs[i], state)\n",
        "            outputs += [out]\n",
        "        return torch.stack(outputs), state\n",
        "\n",
        "\n",
        "class TNNCell(nn.Module):\n",
        "    \"\"\"The main TNN logic. Here, the sub-NNs are initialized as well as the constant learnable\n",
        "    thermal capacitances. The forward function houses the LPTN ODE discretized with the explicit Euler method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sample_time = 0.5  # in s\n",
        "        self.output_size = len(target_cols)\n",
        "        self.caps = TorchParam(torch.Tensor(self.output_size).to(device))\n",
        "        nn.init.normal_(\n",
        "            self.caps, mean=-9.2, std=0.5\n",
        "        )  # hand-picked init mean, might be application-dependent\n",
        "        n_temps = len(temperature_cols)  # number of temperatures (targets and input)\n",
        "        n_conds = int(0.5 * n_temps * (n_temps - 1))  # number of thermal conductances\n",
        "        # conductance net sub-NN\n",
        "        self.conductance_net = nn.Sequential(\n",
        "            nn.Linear(len(input_cols) + self.output_size, n_conds), nn.Sigmoid()\n",
        "        )\n",
        "        # populate adjacency matrix. It is used for indexing the conductance sub-NN output\n",
        "        self.adj_mat = np.zeros((n_temps, n_temps), dtype=int)\n",
        "        adj_idx_arr = np.ones_like(self.adj_mat)\n",
        "        triu_idx = np.triu_indices(n_temps, 1)\n",
        "        adj_idx_arr = adj_idx_arr[triu_idx].ravel()\n",
        "        self.adj_mat[triu_idx] = np.cumsum(adj_idx_arr) - 1\n",
        "        self.adj_mat += self.adj_mat.T\n",
        "        self.adj_mat = torch.from_numpy(self.adj_mat[: self.output_size, :]).type(\n",
        "            torch.int64\n",
        "        )  # crop\n",
        "        self.n_temps = n_temps\n",
        "\n",
        "        # power loss sub-NN\n",
        "        self.ploss = nn.Sequential(\n",
        "            nn.Linear(len(input_cols) + self.output_size, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, self.output_size),\n",
        "        )\n",
        "\n",
        "        self.temp_idcs = [i for i, x in enumerate(input_cols) if x in temperature_cols]\n",
        "        self.nontemp_idcs = [\n",
        "            i\n",
        "            for i, x in enumerate(input_cols)\n",
        "            if x not in temperature_cols + [\"profile_id\"]\n",
        "        ]\n",
        "\n",
        "    def forward(self, inp: Tensor, hidden: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        prev_out = hidden\n",
        "        temps = torch.cat([prev_out, inp[:, self.temp_idcs]], dim=1)\n",
        "        sub_nn_inp = torch.cat([inp, prev_out], dim=1)\n",
        "        conducts = torch.abs(self.conductance_net(sub_nn_inp))\n",
        "        power_loss = torch.abs(self.ploss(sub_nn_inp))\n",
        "        temp_diffs = torch.sum(\n",
        "            (temps.unsqueeze(1) - prev_out.unsqueeze(-1)) * conducts[:, self.adj_mat],\n",
        "            dim=-1,\n",
        "        )\n",
        "        out = prev_out + self.sample_time * torch.exp(self.caps) * (\n",
        "            temp_diffs + power_loss\n",
        "        )\n",
        "        return prev_out, torch.clip(out, -1, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1038587d",
      "metadata": {
        "id": "1038587d"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11740783",
      "metadata": {
        "metadata": {},
        "id": "11740783"
      },
      "outputs": [],
      "source": [
        "model = torch.jit.script(DiffEqLayer(TNNCell).to(device))\n",
        "loss_func = nn.MSELoss(reduction=\"none\")\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "n_epochs = 100\n",
        "tbptt_size = 512\n",
        "\n",
        "n_batches = np.ceil(train_tensor.shape[0] / tbptt_size).astype(int)\n",
        "with tqdm(desc=\"Training\", total=n_epochs) as pbar:\n",
        "    for epoch in range(n_epochs):\n",
        "        # first state is ground truth temperature data\n",
        "        hidden = train_tensor[0, :, -len(target_cols) :]\n",
        "\n",
        "        # propagate batch-wise through data set\n",
        "        for i in range(n_batches):\n",
        "            model.zero_grad()\n",
        "            output, hidden = model(\n",
        "                train_tensor[\n",
        "                    i * tbptt_size : (i + 1) * tbptt_size, :, : len(input_cols)\n",
        "                ],\n",
        "                hidden.detach(),\n",
        "            )\n",
        "            loss = loss_func(\n",
        "                output,\n",
        "                train_tensor[\n",
        "                    i * tbptt_size : (i + 1) * tbptt_size, :, -len(target_cols) :\n",
        "                ],\n",
        "            )\n",
        "            # sample_weighting\n",
        "            loss = (\n",
        "                (\n",
        "                    loss\n",
        "                    * train_sample_weights[\n",
        "                        i * tbptt_size : (i + 1) * tbptt_size, :, None\n",
        "                    ]\n",
        "                    / train_sample_weights[\n",
        "                        i * tbptt_size : (i + 1) * tbptt_size, :\n",
        "                    ].sum()\n",
        "                )\n",
        "                .sum()\n",
        "                .mean()\n",
        "            )\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        # reduce learning rate\n",
        "        if epoch == 75:\n",
        "            for group in opt.param_groups:\n",
        "                group[\"lr\"] *= 0.5\n",
        "        pbar.update()\n",
        "        pbar.set_postfix_str(f\"loss: {loss.item():.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38fa8900",
      "metadata": {
        "id": "38fa8900"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f24f237",
      "metadata": {
        "metadata": {},
        "id": "4f24f237"
      },
      "outputs": [],
      "source": [
        "# model saving and loading\n",
        "mdl_path = Path.cwd() / 'data' / 'models'\n",
        "mdl_path.mkdir(exist_ok=True, parents=True)\n",
        "mdl_file_path = mdl_path / 'tnn_jit_torch.pt'\n",
        "\n",
        "model.save(mdl_file_path)  # save\n",
        "model = torch.jit.load(mdl_file_path)  # load\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388ed64b",
      "metadata": {
        "metadata": {},
        "id": "388ed64b"
      },
      "outputs": [],
      "source": [
        "# evaluate against test set\n",
        "with torch.no_grad():\n",
        "    pred, hidden = model(\n",
        "        test_tensor[:, :, : len(input_cols)], test_tensor[0, :, -len(target_cols) :]\n",
        "    )\n",
        "    pred = pred.cpu().numpy() * 200  # denormalize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e6d35a",
      "metadata": {
        "id": "e7e6d35a"
      },
      "source": [
        "## Visualize Performance\n",
        "We see the performance for one trial, i.e., one run of random initialized weights.\n",
        "\n",
        "Usually, running multiple trials with different random number generator seeds yields even better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85409562",
      "metadata": {
        "metadata": {},
        "id": "85409562"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(len(test_profiles), len(target_cols), figsize=(20, 10))\n",
        "for i, (pid, y_test) in enumerate(\n",
        "    data.loc[data.profile_id.isin(test_profiles), target_cols + [\"profile_id\"]].groupby(\n",
        "        \"profile_id\"\n",
        "    )\n",
        "):\n",
        "    y_test *= 200\n",
        "    profile_pred = pred[: len(y_test), i, :]\n",
        "    for j, col in enumerate(target_cols):\n",
        "        ax = axes[i, j]\n",
        "        ax.plot(\n",
        "            y_test.loc[:, col].reset_index(drop=True),\n",
        "            color=\"tab:green\",\n",
        "            label=\"Ground truth\",\n",
        "        )\n",
        "        ax.plot(profile_pred[:, j], color=\"tab:blue\", label=\"Prediction\")\n",
        "        ax.text(\n",
        "            x=0.5,\n",
        "            y=0.8,\n",
        "            s=f\"MSE: {((profile_pred[:, j] - y_test.loc[:, col])**2).sum() / len(profile_pred):.3f} K²\\nmax.abs.:{(profile_pred[:, j]-y_test.loc[:, col]).abs().max():.1f} K\",\n",
        "            transform=ax.transAxes,\n",
        "        )\n",
        "        if j == 0:\n",
        "            ax.set_ylabel(f\"Profile {pid}\\n Temp. in °C\")\n",
        "            if i == 0:\n",
        "                ax.legend()\n",
        "        if i == len(test_profiles) - 1:\n",
        "            ax.set_xlabel(f\"Iters\")\n",
        "        elif i == 0:\n",
        "            ax.set_title(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce9707f4",
      "metadata": {
        "id": "ce9707f4"
      },
      "source": [
        "The performance that is achievable by the hybridization of LPTNs with neural networks is unprecedented and not achievable by pure LPTN or pure black-box ML models.\n",
        "Note that the visualized performance stems from training a TNN from scratch once. All neural networks are initialized randomly when their training by gradient descent begins.\n",
        "This means that better performance can be easily achieved by repeating this experiment since the convergence into better local minima becomes likely."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d7797872260fffc3150fb09aa844ee2632394704ff9dbe7473c5eb66442c979a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}